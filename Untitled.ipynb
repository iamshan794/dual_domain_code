{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b866a628-0653-4733-916f-fab094624da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ktransformer.py     dataset.py\t      multi_head_attn.py  train.ipynb\n",
      "Log\t\t    fftc.py\t      params.csv\t  train.py\n",
      "README.md\t    generate_mask.py  perform_tl.py\t  utils.py\n",
      "Untitled.ipynb\t    kspace.zip\t      requirements.txt\t  visualisation.ipynb\n",
      "__pycache__\t    layers.py\t      sampling.py\n",
      "data_preprocess.py  main.py\t      test.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7d0a119-1610-48bb-99d4-07c14c96020d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-09 17:39:54.194474: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 17:39:54.781591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Start Loading Dataset from /home/mainuser/datadrive/predata/data/train/hr/numpy_train_hr128.npy, \n",
      "Mask from /home/mainuser/datadrive/predata/masks/Train_gaussian2d_0.2.npy\n",
      "/home/mainuser/datadrive/predata/data/train/hr/numpy_train_hr128.npy /home/mainuser/datadrive/predata/data/train/lr/numpy_train_lr128.npy\n",
      "Train data num : 308\n",
      "valid data num : 48 \n",
      "Sampled Ratio: 0.2499 \n",
      "Dataset load time : 0 \n",
      "\n",
      "Output file locate at : Log/log\n",
      "LIST OF PARAMETERS STORED\n",
      "DataParallel(\n",
      "  (module): Transformer(\n",
      "    (encoder_embed_layer): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (pe_layer): PositionalEncoding()\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoderLR): TransformerDecoderLR(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerDecoderLayerLR(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (multihead_attn): MultiHeadAttention(\n",
      "            (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn1): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (LR_norm_layers): ModuleList(\n",
      "        (0-3): 4 x LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (LR_predict_layers): ModuleList(\n",
      "        (0-3): 4 x Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoderHR): TransformerDecoderHR(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerDecoderLayerHR(\n",
      "          (multihead_attn): MultiHeadAttention(\n",
      "            (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (fc): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn1): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (HR_norm_layers): ModuleList(\n",
      "        (0-5): 6 x LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (HR_predict_layers): ModuleList(\n",
      "        (0-5): 6 x Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (HR_embed_layer): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (ConvBlocks): ModuleList(\n",
      "        (0-5): 6 x CNN_Block(\n",
      "          (convs): ModuleList(\n",
      "            (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "            (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "            (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "            (8): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --output_dir 'log' \\\n",
    "--train_hr_data_path '/home/mainuser/datadrive/predata/data/train/hr/numpy_train_hr128.npy' \\\n",
    "--train_lr_data_path '/home/mainuser/datadrive/predata/data/train/lr/numpy_train_lr128.npy' \\\n",
    "--train_mask_path '/home/mainuser/datadrive/predata/masks/Train_gaussian2d_0.2.npy' \\\n",
    "--valid_hr_data_path '/home/mainuser/datadrive/predata/data/valid/hr/numpy_valid_hr128.npy' \\\n",
    "--valid_lr_data_path '/home/mainuser/datadrive/predata/data/valid/lr/numpy_valid_lr128.npy' \\\n",
    "--valid_mask_path '/home/mainuser/datadrive/predata/masks/Train_gaussian2d_0.2.npy' \\\n",
    "--performTL 'False' \\\n",
    "--get_params 'False' \\\n",
    "--visualize_model 'True' \\\n",
    "--gpu '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6845407f-21a2-405d-b6ea-bd366d216ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  8 22:34:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  Off  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    79W / 300W |  80844MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  Off  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    75W / 300W |   1256MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50a324d-0a76-443a-a418-97ae82713220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/mainuser/.local/lib/python3.8/site-packages (from torchviz) (2.0.0)\n",
      "Requirement already satisfied: graphviz in /home/mainuser/.local/lib/python3.8/site-packages (from torchviz) (0.20.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (10.9.0.58)\n",
      "Requirement already satisfied: jinja2 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.10.3.66)\n",
      "Requirement already satisfied: networkx in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (3.0)\n",
      "Requirement already satisfied: filelock in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (3.10.2)\n",
      "Requirement already satisfied: typing-extensions in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (2.0.0)\n",
      "Requirement already satisfied: sympy in /home/mainuser/.local/lib/python3.8/site-packages (from torch->torchviz) (1.11.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchviz) (67.6.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchviz) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/mainuser/.local/lib/python3.8/site-packages (from triton==2.0.0->torch->torchviz) (3.26.0)\n",
      "Requirement already satisfied: lit in /home/mainuser/.local/lib/python3.8/site-packages (from triton==2.0.0->torch->torchviz) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mainuser/.local/lib/python3.8/site-packages (from sympy->torch->torchviz) (1.3.0)\n",
      "Building wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=0092ec9460167b25f7d3d96bc303503c48ac9e31b2387804ea5389344519bdf0\n",
      "  Stored in directory: /home/mainuser/.cache/pip/wheels/05/7d/1b/8306781244e42ede119edbb053bdcda1c1f424ca226165a417\n",
      "Successfully built torchviz\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a1c10-56f6-449f-8e91-eba0c3363690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
